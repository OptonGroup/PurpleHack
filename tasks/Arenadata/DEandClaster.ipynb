{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49ac5b5-6b63-4732-a383-f102889526c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# import os\n",
    "\n",
    "# # Указываем шаблон пути для файлов PSX\n",
    "# psx_files_pattern = 'telecom100k/telecom100k/telecom100k/psx/*.csv'\n",
    "\n",
    "# # Читаем файлы, указывая dtype для EndSession, чтобы гарантировать, что он читается как объект (строка)\n",
    "# df_dask = dd.read_csv(psx_files_pattern, dtype={'EndSession': 'object'})\n",
    "\n",
    "# # Если столбец с опечаткой 'Duartion' присутствует, переименовываем его в 'Duration'\n",
    "# if 'Duartion' in df_dask.columns:\n",
    "#     df_dask = df_dask.rename(columns={'Duartion': 'Duration'})\n",
    "\n",
    "# # Вычисляем объединённый DataFrame, преобразуя Dask DataFrame в pandas DataFrame\n",
    "# df_combined = df_dask.compute()\n",
    "\n",
    "# # Сохраняем объединённый датасет в один CSV файл\n",
    "# output_file = 'combined_psx.csv'\n",
    "# df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Объединённый датасет сохранён в '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb56acb-ad75-4123-ab3a-0ba5ae6fae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df_combined = pd.read_csv('combined_psx.csv')\n",
    "# df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a138070-5a48-470f-9c60-0b49869cbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Чтение файла с подписчиками (Subscriber)\n",
    "# subscriber_file = 'telecom100k/telecom100k/telecom100k/subscribers.csv'\n",
    "# df_subscriber = pd.read_csv(subscriber_file, dtype={'IdOnPSX': str, 'IdClient': str})\n",
    "\n",
    "# # 3. Приведение типов для объединения\n",
    "# df_combined['IdSubscriber'] = df_combined['IdSubscriber'].astype(str)\n",
    "# df_subscriber['IdOnPSX'] = df_subscriber['IdOnPSX'].astype(str)\n",
    "\n",
    "# # 4. Объединение по полям: PSXStats.IdSubscriber = Subscriber.IdOnPSX\n",
    "# df_merged = pd.merge(df_combined, df_subscriber[['IdClient', 'IdOnPSX']], \n",
    "#                      left_on='IdSubscriber', right_on='IdOnPSX', how='left')\n",
    "\n",
    "# # Убираем из итогового DataFrame лишний столбец (IdOnPSX) из subscriber\n",
    "# df_merged.drop(columns=['IdOnPSX'], inplace=True)\n",
    "\n",
    "# # 5. Сохраняем результат в новый CSV-файл\n",
    "# output_file = 'combined_psx_with_client.csv'\n",
    "# df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Новый датасет с добавленным IdClient сохранён в '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09d6111-8c78-4d97-bef0-1e51092bb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = pd.read_csv('combined_psx_with_client.csv')\n",
    "# df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6aeea4-b263-48af-b67c-fe425797a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Создаем папку для сохранения файлов, если её нет\n",
    "# output_dir = 'clients'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Группируем данные по IdClient и сохраняем каждую группу в отдельный CSV-файл\n",
    "# for client_id, group in df_merged.groupby('IdClient'):\n",
    "#     # Сортировка по времени, если требуется (по StartSession)\n",
    "#     group_sorted = group.sort_values('StartSession')\n",
    "#     # Формируем имя файла для клиента\n",
    "#     file_path = os.path.join(output_dir, f'client_{client_id}.csv')\n",
    "#     group_sorted.to_csv(file_path, index=False)\n",
    "\n",
    "# print(\"Датасет успешно разбит на группы по IdClient, файлы сохранены в папке 'clients'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331217ca-add1-490b-a7c7-cb200bc13fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # 1. Считываем исходный файл клиентов (client.parquet)\n",
    "# # Здесь содержится информация о клиенте, включая идентификатор клиента и его тарифный план (IdPlan)\n",
    "# clients_file = 'telecom100k/telecom100k/telecom100k/client.parquet'\n",
    "# df_clients_orig = pd.read_parquet(clients_file)\n",
    "# # Приводим идентификатор клиента к строковому типу и переименовываем для удобства объединения\n",
    "# df_clients_orig['Id'] = df_clients_orig['Id'].astype(str)\n",
    "# df_client_mapping = df_clients_orig[['Id', 'IdPlan']].copy()\n",
    "# df_client_mapping.rename(columns={'Id': 'IdClient'}, inplace=True)\n",
    "# df_client_mapping['IdPlan'] = df_client_mapping['IdPlan'].astype(str)\n",
    "\n",
    "# # 2. Считываем данные тарифных планов (Plan) из JSON\n",
    "# plan_file = 'telecom100k/telecom100k/telecom100k/plan.json'\n",
    "# df_plan = pd.read_json(plan_file)\n",
    "# df_plan['Id'] = df_plan['Id'].astype(str)  # тарифный план хранится в столбце 'Id' в df_plan\n",
    "\n",
    "# # 3. Считываем данные параметров коммутаторов (PSXAttrs) из CSV\n",
    "# psxattrs_file = 'telecom100k/telecom100k/telecom100k/psxattrs.csv'\n",
    "# df_psxattrs = pd.read_csv(psxattrs_file, dtype={'Id': str})\n",
    "# # Если нужно, можно оставить только интересующие столбцы:\n",
    "# # df_psxattrs = df_psxattrs[['Id', 'PSX', 'TransmitUnits', 'Delimiter', 'DateFormat', 'TZ']]\n",
    "\n",
    "# # 4. Находим все файлы клиентов в папке \"clients\"\n",
    "# client_files = glob.glob(os.path.join('clients', 'client_*.csv'))\n",
    "\n",
    "# # 5. Обрабатываем каждый файл\n",
    "# for file in client_files:\n",
    "#     # Считываем файл клиента. Файл содержит: IdSession, IdPSX, IdSubscriber, StartSession, EndSession, Duration, UpTx, DownTx, IdClient\n",
    "#     df_client = pd.read_csv(file, dtype={'IdClient': str, 'IdPSX': str})\n",
    "    \n",
    "#     # 5.1. Объединяем с клиентским маппингом для добавления поля IdPlan\n",
    "#     df_client = pd.merge(df_client, df_client_mapping, on='IdClient', how='left')\n",
    "    \n",
    "#     # 5.2. Объединяем с тарифными планами по полю IdPlan (df_client.IdPlan из маппинга = df_plan.Id)\n",
    "#     df_client = pd.merge(df_client, df_plan, left_on='IdPlan', right_on='Id', how='left', suffixes=('', '_plan'))\n",
    "#     # Если лишний столбец 'Id_plan' появился, его можно удалить (но здесь основное поле тарифного плана – 'IdPlan' уже добавлено)\n",
    "    \n",
    "#     # 5.3. Объединяем с параметрами коммутаторов (PSXAttrs) по полю IdPSX\n",
    "#     df_client = pd.merge(df_client, df_psxattrs, left_on='IdPSX', right_on='Id', how='left', suffixes=('', '_psxattrs'))\n",
    "#     # При необходимости можно удалить дублирующий столбец, например, 'Id_psxattrs'\n",
    "#     if 'Id_psxattrs' in df_client.columns:\n",
    "#         df_client.drop(columns=['Id_psxattrs'], inplace=True)\n",
    "    \n",
    "#     # 5.4. Сохраняем обновлённый DataFrame обратно в тот же файл\n",
    "#     df_client.to_csv(file, index=False)\n",
    "\n",
    "# print(\"Для каждого файла клиента добавлена информация о тарифном плане и PSXAttrs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab465440-f619-4449-a36c-94b976468862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_client_data(df):\n",
    "    # Если в колонке TransmitUnits указано 'bytes', умножаем UpTx и DownTx на 8\n",
    "    mask = df['TransmitUnits'] == 'bytes'\n",
    "    df.loc[mask, 'UpTx'] = df.loc[mask, 'UpTx'] * 8\n",
    "    df.loc[mask, 'DownTx'] = df.loc[mask, 'DownTx'] * 8\n",
    "\n",
    "    # Функция для расчёта среднего количества пакетов за длительность\n",
    "    def compute_avg_packets(row):\n",
    "        if row['Duration'] <= 0:\n",
    "            return 0\n",
    "        return (row['UpTx'] + row['DownTx']) / row['Duration']\n",
    "    \n",
    "    # Применяем функцию и создаём столбец 'AvgPackets'\n",
    "    df['AvgPackets'] = df.apply(compute_avg_packets, axis=1)\n",
    "\n",
    "    # Функция для расчёта среднего исходящего трафика\n",
    "    def compute_avg_up(row):\n",
    "        if row['Duration'] <= 0:\n",
    "            return 0\n",
    "        return row['UpTx'] / row['Duration']\n",
    "    \n",
    "    # Функция для расчёта среднего входящего трафика\n",
    "    def compute_avg_down(row):\n",
    "        if row['Duration'] <= 0:\n",
    "            return 0\n",
    "        return row['DownTx'] / row['Duration']\n",
    "    \n",
    "    # Создаём столбцы 'AvgUp' и 'AvgDown'\n",
    "    df['AvgUp'] = df.apply(compute_avg_up, axis=1)\n",
    "    df['AvgDown'] = df.apply(compute_avg_down, axis=1)\n",
    "\n",
    "    # Функция для извлечения второго значения из строки в колонке Attrs\n",
    "    def get_second_attr(row):\n",
    "        # Если Attrs или Delimiter отсутствуют, возвращаем None\n",
    "        if pd.isna(row['Attrs']) or pd.isna(row['Delimiter']):\n",
    "            return None\n",
    "        parts = row['Attrs'].split(row['Delimiter'])\n",
    "        # Проверяем, что есть как минимум два элемента\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Добавляем новый столбец с вторым значением\n",
    "    df['EthernetPlan'] = df.apply(get_second_attr, axis=1)\n",
    "    df = df.drop(columns=['Attrs', 'TransmitUnits', 'PSX', 'Delimiter', 'Description'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "# Пример использования:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# df = process_client_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36ceddd-bbc4-4288-bc57-58d260771841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def convert_to_utc(date_str, date_format, tz_str):\n",
    "    \"\"\"\n",
    "    Парсит дату по date_format и локализует по временной зоне из tz_str (например, 'GMT-6'),\n",
    "    затем конвертирует время в UTC.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    dt = datetime.strptime(date_str, date_format)\n",
    "    offset = int(tz_str.replace(\"GMT\", \"\"))  # извлекаем смещение (например, -6)\n",
    "    dt = dt.replace(tzinfo=timezone(timedelta(hours=offset)))\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def process_session_columns(df):\n",
    "    \"\"\"\n",
    "    1. Добавляет в DataFrame колонки StartSessionUTC и EndSessionUTC (переведённые в UTC).\n",
    "    2. Создаёт колонку SessionDuration:\n",
    "       - Если EndSessionUTC != 0, вычисляет разницу (в секундах) между EndSessionUTC и StartSessionUTC.\n",
    "       - Если EndSessionUTC == 0 (EndSession NaN), берёт исходный Duration.\n",
    "    3. Удаляет оригинальные колонки StartSession, EndSession, TZ, DateFormat.\n",
    "    \"\"\"\n",
    "\n",
    "    # Преобразуем StartSession в UTC\n",
    "    df['StartSessionUTC'] = df.apply(\n",
    "        lambda row: convert_to_utc(row['StartSession'], row['DateFormat'], row['TZ']), axis=1\n",
    "    )\n",
    "    # Преобразуем EndSession в UTC или ставим 0\n",
    "    df['EndSessionUTC'] = df.apply(\n",
    "        lambda row: convert_to_utc(row['EndSession'], row['DateFormat'], row['TZ'])\n",
    "        if pd.notna(row['EndSession']) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Функция для вычисления фактической длительности сессии\n",
    "    def compute_session_duration(row):\n",
    "        # Если EndSessionUTC == 0, используем исходный Duration\n",
    "        if row['EndSessionUTC'] == 0 or row['StartSessionUTC'] is None:\n",
    "            return row['Duration']\n",
    "        else:\n",
    "            # Вычисляем разницу во времени (в секундах)\n",
    "            return (row['EndSessionUTC'] - row['StartSessionUTC']).total_seconds()\n",
    "\n",
    "    # Добавляем новую колонку с рассчитанной длительностью\n",
    "    df['SessionDuration'] = df.apply(compute_session_duration, axis=1)\n",
    "\n",
    "    # Удаляем ненужные колонки\n",
    "    df = df.drop(columns=['StartSession', 'EndSession', 'TZ', 'Duration'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a23139-2167-48fe-876d-1be2e1ce8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_endpoint_column(df):\n",
    "    \"\"\"\n",
    "    Функция принимает DataFrame с колонками:\n",
    "      - StartSessionUTC: строковое представление даты/времени,\n",
    "      - DateFormat: формат даты/времени (например, '%Y-%m-%d %H:%M:%S'),\n",
    "      - duration: целое число секунд.\n",
    "      \n",
    "    Функция вычисляет новую колонку endpoint:\n",
    "      endpoint = (StartSessionUTC + duration) в виде количества секунд с эпохи Unix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calc_endpoint(row):\n",
    "        # Преобразуем StartSessionUTC в datetime с использованием указанного формата\n",
    "        start_dt = pd.to_datetime(row['StartSessionUTC'], format=row['DateFormat'])\n",
    "        # Прибавляем к дате duration в секундах\n",
    "        end_dt = start_dt + pd.Timedelta(seconds=row['SessionDuration'])\n",
    "        # Преобразуем итоговую дату в Unix timestamp (секунды с 1970-01-01)\n",
    "        return int(end_dt.timestamp())\n",
    "    \n",
    "    # Применяем функцию построчно\n",
    "    df['EndPoint'] = df.apply(calc_endpoint, axis=1)\n",
    "    df = df.drop(columns=['EndSessionUTC', 'DateFormat'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b86f4-2ef3-4e68-9702-aad7ecb97720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_graph(df, x, y, bins=10, smoothing_window=6):\n",
    "    \"\"\"\n",
    "    Функция строит график по заданным параметрам:\n",
    "      - Сглаживает значения y с помощью скользящего среднего.\n",
    "      - Отображает scatter-график сглаженных данных.\n",
    "      - Строит непрерывную линию, показывающую медианные значения сглаженных y,\n",
    "        рассчитанные по оконным (биновым) интервалам оси x.\n",
    "    \n",
    "    Параметры:\n",
    "      df : pandas.DataFrame\n",
    "          Датафрейм с данными.\n",
    "      x : str\n",
    "          Имя столбца для оси X (числовой или datetime).\n",
    "      y : str\n",
    "          Имя столбца для оси Y.\n",
    "      bins : int, optional\n",
    "          Количество бинов (окон) для расчёта медианы. По умолчанию 10.\n",
    "      smoothing_window : int, optional\n",
    "          Размер окна для сглаживания y (скользящее среднее). По умолчанию 5.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Сглаживаем значения y\n",
    "    df['y_smooth'] = df[y].rolling(window=smoothing_window, min_periods=1, center=True).mean()\n",
    "    \n",
    "    # Строим scatter-график сглаженных значений\n",
    "    plt.scatter(df[x], df['y_smooth'], marker='o', label='Сглаженные данные')\n",
    "    \n",
    "    # Определяем границы бинов по оси x\n",
    "    bin_edges = np.linspace(df[x].min(), df[x].max(), bins+1)\n",
    "    \n",
    "    # Группируем данные по созданным бинам\n",
    "    df['bin'] = pd.cut(df[x], bins=bin_edges, include_lowest=True)\n",
    "    \n",
    "    # Вычисляем медианные значения сглаженных y для каждого бина\n",
    "    median_df = df.groupby('bin', observed=False)['y_smooth'].median()\n",
    "    \n",
    "    # Вычисляем центры каждого бина\n",
    "    bin_centers = [interval.mid for interval in median_df.index.categories]\n",
    "    \n",
    "    # Строим линию медианных значений\n",
    "    plt.plot(bin_centers, median_df.values, color='red', marker='o', linestyle='-',\n",
    "             label='Медиана по окнам (сглаженные)')\n",
    "    \n",
    "    # Вычисляем общую медиану сглаженных значений\n",
    "    overall_median = df['y_smooth'].median()\n",
    "    \n",
    "    # Добавляем горизонтальную линию для общей медианы\n",
    "    plt.axhline(y=overall_median, color='green', linestyle='--', label='Общая медиана (сглаженные)')\n",
    "    \n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.title(f\"График сглаженных значений {y} по {x}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4601c93-9e86-4710-b680-3204bb2a5fed",
   "metadata": {},
   "source": [
    "UpTx – это объём трафика, который передается от абонента (исходящий трафик).\n",
    "DownTx – это объём трафика, который принимается абонентом (входящий трафик).\n",
    "\n",
    "Для первичного анализа рекомендуется начать с UpTx.\n",
    "Причина в том, что у обычных пользователей объём исходящего трафика часто значительно ниже, \n",
    "чем входящего, и любое резкое увеличение этого показателя может указывать на несанкционированную активность \n",
    "(например, утечку данных или взлом). Сравнивая ретроспективное значение UpTx с текущим, можно более чётко выявить аномалии в поведении абонента. \n",
    "После этого целесообразно проанализировать и DownTx для комплексной оценки ситуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cce398-aa64-4305-a271-391a720327d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_clients_files(folder_path):\n",
    "    \"\"\"\n",
    "    Функция проходит по всем файлам в папке folder_path.\n",
    "    Для каждого файла пытается загрузить данные в DataFrame и выполнить дальнейшую обработку.\n",
    "    \n",
    "    Параметры:\n",
    "      folder_path : str\n",
    "          Путь к папке, содержащей файлы клиентов.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # Получаем список всех файлов в папке\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        print(f\"Найдено файлов: {len(files)}\")\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            count += 1\n",
    "            # Рисуем график только для каждого сотого файла\n",
    "            if count % 100 == 0:\n",
    "                try:\n",
    "                    client = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Удаление лишних столбцов\n",
    "                    client = client.drop(columns=['IdPSX', 'IdSubscriber', 'Name', 'Id', 'CreatedAt', 'CreatedAt', 'UpdatedAt', 'ClosedAt'])\n",
    "                    \n",
    "                    # Обработка данных: функции должны быть определены заранее\n",
    "                    client = process_client_data(client)\n",
    "                    client = process_session_columns(client)\n",
    "                    client = add_endpoint_column(client)\n",
    "                    \n",
    "                    # Масштабирование столбцов 'EndPoint' и 'AvgPackets' в диапазоне от 0 до 1\n",
    "                    for col in ['EndPoint', 'AvgPackets']:\n",
    "                        min_val = client[col].min()\n",
    "                        max_val = client[col].max()\n",
    "                        # Если все значения равны, оставляем их без изменений, чтобы избежать деления на 0\n",
    "                        if max_val != min_val:\n",
    "                            client[col] = (client[col] - min_val) / (max_val - min_val)\n",
    "                    \n",
    "                    print(f'Обрабатываем файл {file_path}')\n",
    "                    # Построение графика\n",
    "                    build_graph(client, 'EndPoint', 'AvgPackets', bins=20, smoothing_window=5)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
    "\n",
    "# Пример вызова функции:\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"clients\"  # путь к папке с файлами\n",
    "    process_clients_files(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78095637-c71f-4f46-a919-f5bec693bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_csv('clients/client_6bc922bf-d0c8-45bc-85a5-10bf446a00e1.csv')\n",
    "\n",
    "client = client.drop(columns=['IdPSX', 'IdSubscriber', 'Name', 'Id', 'CreatedAt', 'CreatedAt', 'UpdatedAt', 'ClosedAt'])\n",
    "client = process_client_data(client)\n",
    "client = process_session_columns(client)\n",
    "client = add_endpoint_column(client)\n",
    "\n",
    "client.drop(columns=['IdPlan', 'IdClient', 'IdSession', 'StartSessionUTC', 'UpTx', 'DownTx', 'Enabled', 'EthernetPlan', 'SessionDuration']).sort_values('EndPoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd9c7b-2eeb-4b1f-9337-6fc244d251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Допустим, у вас уже есть DataFrame client\n",
    "# где вы заранее удалили ненужные столбцы, например:\n",
    "# client.drop(columns=['IdPlan','IdClient','IdSession','StartSessionUTC'], inplace=True, errors='ignore')\n",
    "\n",
    "# 1. Формируем матрицу признаков\n",
    "features = client[['AvgPackets', 'AvgUp', 'AvgDown', 'EndPoint']].copy()\n",
    "\n",
    "# 2. Обучаем Isolation Forest\n",
    "# Параметр contamination задаёт долю предполагаемых аномалий (например, 5%)\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "model.fit(features)\n",
    "\n",
    "# 3. Получаем предсказания (-1 — аномалия, 1 — нормальная точка)\n",
    "predictions = model.predict(features)\n",
    "\n",
    "# 4. Добавляем колонку с флагом аномалии в исходный DataFrame\n",
    "client['anomaly'] = (predictions == -1)\n",
    "\n",
    "# 5. Визуализируем результат\n",
    "# Для примера построим Scatter-график по осям: EndPoint (X) и AvgPackets (Y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Нормальные точки\n",
    "plt.scatter(\n",
    "    client.loc[client['anomaly'] == False, 'EndPoint'],\n",
    "    client.loc[client['anomaly'] == False, 'AvgPackets'],\n",
    "    c='blue',\n",
    "    label='Normal'\n",
    ")\n",
    "\n",
    "# Аномальные точки\n",
    "plt.scatter(\n",
    "    client.loc[client['anomaly'] == True, 'EndPoint'],\n",
    "    client.loc[client['anomaly'] == True, 'AvgPackets'],\n",
    "    c='red',\n",
    "    label='Anomaly'\n",
    ")\n",
    "\n",
    "plt.xlabel('EndPoint')\n",
    "plt.ylabel('AvgPackets')\n",
    "plt.title('Isolation Forest - Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b7afa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('telecom100k/telecom100k/telecom100k/RESULT', sep=',')\n",
    "client = pd.read_csv('clients/client_96969c99-55cc-4cb2-8227-063426eb6ee1.csv')\n",
    "IdS = result['UID'].unique()  # Получаем уникальные значения Id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def process_clients_files(folder_path, files):\n",
    "    \"\"\"\n",
    "    Функция проходит по всем файлам в папке folder_path.\n",
    "    Для каждого файла загружает данные, выполняет обработку, обнаружение аномалий\n",
    "    с использованием Isolation Forest и строит график.\n",
    "    \n",
    "    Параметры:\n",
    "      folder_path : str\n",
    "          Путь к папке, содержащей файлы клиентов.\n",
    "      files : list\n",
    "          Список идентификаторов/имен файлов (без расширения).\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, f'client_{file}.csv')\n",
    "        try:\n",
    "            # Загрузка данных\n",
    "            client = pd.read_csv(file_path)\n",
    "            \n",
    "            # Удаление лишних столбцов\n",
    "            client = client.drop(columns=['IdPSX', 'IdSubscriber', 'Name', 'Id', \n",
    "                                           'CreatedAt', 'UpdatedAt', 'ClosedAt'], errors='ignore')\n",
    "            \n",
    "            # Предварительная обработка данных (функции должны быть определены заранее)\n",
    "            client = process_client_data(client)\n",
    "            client = process_session_columns(client)\n",
    "            client = add_endpoint_column(client)\n",
    "            \n",
    "            # Масштабирование столбцов 'EndPoint' и 'AvgPackets' в диапазоне [0, 1]\n",
    "            for col in ['EndPoint', 'AvgPackets']:\n",
    "                min_val = client[col].min()\n",
    "                max_val = client[col].max()\n",
    "                if max_val != min_val:\n",
    "                    client[col] = (client[col] - min_val) / (max_val - min_val)\n",
    "            \n",
    "            print(f'Обрабатываем файл {file_path}')\n",
    "            \n",
    "            # === Обнаружение аномалий с Isolation Forest ===\n",
    "            # 1. Формирование матрицы признаков\n",
    "            features = client[['AvgPackets', 'AvgUp', 'AvgDown', 'EndPoint']].copy()\n",
    "            \n",
    "            # 2. Обучение модели Isolation Forest\n",
    "            model = IsolationForest(contamination=0.05, random_state=42)\n",
    "            model.fit(features)\n",
    "            \n",
    "            # 3. Получение предсказаний (-1 — аномалия, 1 — нормальная точка)\n",
    "            predictions = model.predict(features)\n",
    "            \n",
    "            # 4. Добавление колонки с флагом аномалии\n",
    "            client['anomaly'] = (predictions == -1)\n",
    "            \n",
    "            # === Визуализация результатов ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Нормальные точки\n",
    "            plt.scatter(\n",
    "                client.loc[client['anomaly'] == False, 'EndPoint'],\n",
    "                client.loc[client['anomaly'] == False, 'AvgPackets'],\n",
    "                c='blue',\n",
    "                label='Normal'\n",
    "            )\n",
    "            \n",
    "            # Аномальные точки\n",
    "            plt.scatter(\n",
    "                client.loc[client['anomaly'] == True, 'EndPoint'],\n",
    "                client.loc[client['anomaly'] == True, 'AvgPackets'],\n",
    "                c='red',\n",
    "                label='Anomaly'\n",
    "            )\n",
    "            \n",
    "            plt.xlabel('EndPoint')\n",
    "            plt.ylabel('AvgPackets')\n",
    "            plt.title(f'Isolation Forest - Anomaly Detection for {file}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
    "\n",
    "# Пример использования:\n",
    "folder = \"clients\"  # путь к папке с файлами\n",
    "process_clients_files(folder, IdS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
