{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10962139,"sourceType":"datasetVersion","datasetId":6820041},{"sourceId":280341,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":240174,"modelId":261819}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:16.685659Z","iopub.execute_input":"2025-03-10T00:22:16.685960Z","iopub.status.idle":"2025-03-10T00:22:20.068891Z","shell.execute_reply.started":"2025-03-10T00:22:16.685930Z","shell.execute_reply":"2025-03-10T00:22:20.068189Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Определение путей к данным\n# CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nDATA_DIR = '/kaggle/input/colors/dataset_colors'\nTRAIN_DATA_DIR = '/kaggle/input/colors/dataset_colors/train_data'\nTEST_DATA_DIR = '/kaggle/input/colors/dataset_colors/test_data'\nTRAIN_CSV = '/kaggle/input/colors/dataset_colors/train_data.csv'\nTEST_CSV = '/kaggle/input/colors/dataset_colors/test_data.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:20.070265Z","iopub.execute_input":"2025-03-10T00:22:20.070713Z","iopub.status.idle":"2025-03-10T00:22:20.074207Z","shell.execute_reply.started":"2025-03-10T00:22:20.070688Z","shell.execute_reply":"2025-03-10T00:22:20.073589Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"TRANSLIT_TO_RU = {\n    'bezhevyi': 'бежевый',\n    'belyi': 'белый',\n    'biryuzovyi': 'бирюзовый',\n    'bordovyi': 'бордовый',\n    'goluboi': 'голубой',\n    'zheltyi': 'желтый',\n    'zelenyi': 'зеленый',\n    'zolotoi': 'золотой',\n    'korichnevyi': 'коричневый',\n    'krasnyi': 'красный',\n    'oranzhevyi': 'оранжевый',\n    'raznocvetnyi': 'разноцветный',\n    'rozovyi': 'розовый',\n    'serebristyi': 'серебряный',\n    'seryi': 'серый',\n    'sinii': 'синий',\n    'fioletovyi': 'фиолетовый',\n    'chernyi': 'черный'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:20.075239Z","iopub.execute_input":"2025-03-10T00:22:20.075491Z","iopub.status.idle":"2025-03-10T00:22:20.099633Z","shell.execute_reply.started":"2025-03-10T00:22:20.075457Z","shell.execute_reply":"2025-03-10T00:22:20.098835Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Маппинг цветов\nCOLORS = {\n    'бежевый': 'beige',\n    'белый': 'white',\n    'бирюзовый': 'turquoise',\n    'бордовый': 'burgundy',\n    'голубой': 'blue',\n    'желтый': 'yellow',\n    'зеленый': 'green',\n    'золотой': 'gold',\n    'коричневый': 'brown',\n    'красный': 'red',\n    'оранжевый': 'orange',\n    'разноцветный': 'variegated',\n    'розовый': 'pink',\n    'серебряный': 'silver',\n    'серый': 'gray',\n    'синий': 'blue',\n    'фиолетовый': 'purple',\n    'черный': 'black'\n}\n\nCATEGORIES = ['одежда для девочек', 'столы', 'стулья', 'сумки']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:20.100532Z","iopub.execute_input":"2025-03-10T00:22:20.100822Z","iopub.status.idle":"2025-03-10T00:22:20.117598Z","shell.execute_reply.started":"2025-03-10T00:22:20.100793Z","shell.execute_reply":"2025-03-10T00:22:20.116879Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None, is_test=False):\n        self.df = df.reset_index(drop=True)  # Сбрасываем индексы после фильтрации\n        self.img_dir = img_dir\n        self.transform = transform\n        self.is_test = is_test\n        self.category_to_idx = {cat: idx for idx, cat in enumerate(CATEGORIES)}\n        self.color_to_idx = {color: idx for idx, color in enumerate(COLORS.keys())}\n        \n        # Проверяем все пути к изображениям заранее\n        self.valid_indices = []\n        for idx in range(len(df)):\n            img_path = os.path.join(self.img_dir, f\"{df.iloc[idx]['id']}.jpg\")\n            if os.path.exists(img_path):\n                self.valid_indices.append(idx)\n        \n        if len(self.valid_indices) == 0:\n            raise ValueError(f\"Не найдено ни одного изображения в директории {img_dir}\")\n        \n        print(f\"Найдено {len(self.valid_indices)} валидных изображений из {len(df)}\")\n        \n    def __len__(self):\n        return len(self.valid_indices)\n    \n    def __getitem__(self, idx):\n        real_idx = self.valid_indices[idx]\n        img_id = self.df.iloc[real_idx]['id']\n        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n        \n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Ошибка при загрузке изображения {img_path}: {str(e)}\")\n            raise\n        \n        if self.transform:\n            try:\n                image = self.transform(image)\n            except Exception as e:\n                print(f\"Ошибка при применении трансформации к {img_path}: {str(e)}\")\n                raise\n            \n        category = self.category_to_idx[self.df.iloc[real_idx]['category']]\n        \n        if not self.is_test:\n            color_translit = self.df.iloc[real_idx]['target']\n            color_ru = TRANSLIT_TO_RU[color_translit]\n            color = self.color_to_idx[color_ru]\n            return image, category, color\n        \n        return image, category, img_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:20.118452Z","iopub.execute_input":"2025-03-10T00:22:20.118712Z","iopub.status.idle":"2025-03-10T00:22:20.141113Z","shell.execute_reply.started":"2025-03-10T00:22:20.118679Z","shell.execute_reply":"2025-03-10T00:22:20.140520Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoProcessor, AutoModel, AutoTokenizer, SiglipImageProcessor\n\nclass ColorClassifier(nn.Module):\n    def __init__(self, num_colors, num_categories):\n        super(ColorClassifier, self).__init__()\n        \n        ckpt = \"google/siglip2-base-patch16-384\"\n        # Image embedding model\n        self.image_processor = SiglipImageProcessor.from_pretrained(ckpt)\n        self.image_model = AutoModel.from_pretrained(ckpt, device_map=\"auto\")\n\n        # Text embedding model for categories\n        self.text_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n        self.text_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n\n        # Classification head\n        combined_features_size = 768 + 312  # Image embeddings (1152) + text embeddings (312)\n        self.score = torch.nn.Sequential(\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(combined_features_size, combined_features_size // 2),\n            torch.nn.Dropout(0.1),\n            torch.nn.GELU(),\n            torch.nn.Linear(combined_features_size // 2, num_colors),\n        )\n\n    def embed_image(self, images):\n        inputs = self.image_processor(images=images, return_tensors=\"pt\").to(self.image_model.device)\n        with torch.no_grad():\n            embeddings = self.image_model.get_image_features(**inputs)\n        return embeddings\n\n    def embed_text(self, categories):\n        if isinstance(categories, torch.Tensor):  # Если это тензор, конвертируем в список строк\n            categories = categories.tolist()\n        \n        if isinstance(categories, list):  # Если список, приводим к строковому виду\n            categories = [str(cat) for cat in categories]\n    \n        t = self.text_tokenizer(categories, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self.text_model(**{k: v.to(self.text_model.device) for k, v in t.items()})\n        embeddings = model_output.last_hidden_state[:, 0, :]\n        embeddings = torch.nn.functional.normalize(embeddings)\n        return embeddings\n\n\n    def forward(self, images, categories):\n        image_emb = self.embed_image(images)\n        text_emb = self.embed_text(categories)\n\n        # Combine image and text embeddings\n        combined = torch.cat([image_emb, text_emb], dim=1)\n\n        return self.score(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:20.141888Z","iopub.execute_input":"2025-03-10T00:22:20.142087Z","iopub.status.idle":"2025-03-10T00:22:23.457045Z","shell.execute_reply.started":"2025-03-10T00:22:20.142069Z","shell.execute_reply":"2025-03-10T00:22:23.456363Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport copy\n\n# Импортируем метрики из scikit-learn\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef train_model(\n    model,\n    train_loader,\n    val_loader,\n    criterion,\n    optimizer,\n    scheduler,\n    num_epochs=25\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Используется устройство: {device}\")\n    \n    model = model.to(device)\n    criterion = criterion.to(device)\n    \n    # Для отслеживания лучшей F1 и лучшей модели\n    best_f1 = 0.0\n    best_model_state = None\n    best_epoch = 0\n    \n    # Для логирования динамики обучения\n    # Можно расширять этот словарь под свои нужды\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_precision\": [],\n        \"val_recall\": [],\n        \"val_f1\": [],\n        \"val_accuracy\": []\n    }\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        \n        # -- ТРЕНИРОВОЧНЫЙ ЦИКЛ --\n        for images, categories, colors in tqdm(train_loader,\n                                               desc=f\"Epoch {epoch+1}/{num_epochs}\",\n                                               leave=False):\n            images = images.to(device)\n            categories = categories.to(device)\n            colors = colors.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images, categories)\n            loss = criterion(outputs, colors)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Подсчёт среднего train loss за эпоху\n        train_loss /= len(train_loader)\n        \n        # -- ВАЛИДАЦИЯ --\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_true = []\n        \n        with torch.no_grad():\n            for images, categories, colors in val_loader:\n                images = images.to(device)\n                categories = categories.to(device)\n                colors = colors.to(device)\n                \n                outputs = model(images, categories)\n                loss = criterion(outputs, colors)\n                val_loss += loss.item()\n                \n                # Получаем предсказанные классы\n                preds = torch.argmax(outputs, dim=1)\n                \n                # Сохраняем предсказания и истинные метки для метрик\n                val_preds.extend(preds.cpu().numpy())\n                val_true.extend(colors.cpu().numpy())\n        \n        # Подсчёт среднего val loss за эпоху\n        val_loss /= len(val_loader)\n        \n        # -- МЕТРИКИ --\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            val_true, val_preds, average='macro'\n        )\n        accuracy = accuracy_score(val_true, val_preds)\n        \n        # Сохраняем результаты в history\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_precision\"].append(precision)\n        history[\"val_recall\"].append(recall)\n        history[\"val_f1\"].append(f1)\n        history[\"val_accuracy\"].append(accuracy)\n        \n        # Выводим краткую статистику по эпохе\n        print(\n            f\"Эпоха [{epoch+1}/{num_epochs}] | \"\n            f\"Train Loss: {train_loss:.4f} | \"\n            f\"Val Loss: {val_loss:.4f} | \"\n            f\"Val Precision (macro): {precision:.4f} | \"\n            f\"Val Recall (macro): {recall:.4f} | \"\n            f\"Val Accuracy: {accuracy:.4f} | \"\n            f\"Val F1 (macro): {f1:.4f}\"\n        )\n        \n        # Если F1 улучшилась, сохраняем \"лучшую\" модель\n        if f1 > best_f1:\n            best_f1 = f1\n            best_model_state = copy.deepcopy(model.state_dict())\n            best_epoch = epoch\n            \n            # Сохраняем чекпойнт\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n                'precision': precision,\n                'recall': recall,\n                'f1_score': f1,\n                'accuracy': accuracy,\n                # Если нужно, сохраняйте и размеры словарей, и другие параметры\n                # 'num_colors': len(COLORS),\n                # 'num_categories': len(CATEGORIES)\n            }, 'best_model.pth')\n            print(f'Сохранена лучшая модель с F1: {f1:.4f} на эпохе {epoch+1}')\n        \n        # Делаем шаг шедулера (если требуется)\n        scheduler.step()\n        \n    print(f\"Лучшая F1: {best_f1:.4f} на эпохе {best_epoch+1}\")\n    return best_model_state, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:23.457802Z","iopub.execute_input":"2025-03-10T00:22:23.458238Z","iopub.status.idle":"2025-03-10T00:22:23.470106Z","shell.execute_reply.started":"2025-03-10T00:22:23.458215Z","shell.execute_reply":"2025-03-10T00:22:23.469253Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def load_model(model_path):\n    \"\"\"Загрузка сохраненной модели\"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Файл модели не найден: {model_path}\")\n        \n    checkpoint = torch.load(model_path)\n    model = ColorClassifier(\n        num_colors=checkpoint['num_colors'],\n        num_categories=checkpoint['num_categories']\n    )\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:23.472612Z","iopub.execute_input":"2025-03-10T00:22:23.472881Z","iopub.status.idle":"2025-03-10T00:22:23.497068Z","shell.execute_reply.started":"2025-03-10T00:22:23.472849Z","shell.execute_reply":"2025-03-10T00:22:23.496425Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def predict(model, test_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Используется устройство: {device}\")\n    \n    model = model.to(device)\n    model.eval()\n    \n    predictions = []\n    ids = []\n    \n    color_list = list(COLORS.keys())\n    \n    with torch.no_grad():\n        for images, categories, img_ids in test_loader:\n            images = images.to(device)\n            categories = categories.to(device)\n            \n            outputs = model(images, categories)\n            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n            \n            for img_id, prob in zip(img_ids, probs):\n                pred_dict = {color: float(p) for color, p in zip(color_list, prob)}\n                pred_color = color_list[np.argmax(prob)]\n                \n                predictions.append({\n                    'id': img_id,\n                    'predict_proba': json.dumps(pred_dict),\n                    'predict_color': pred_color\n                })\n                \n    return pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:23.498255Z","iopub.execute_input":"2025-03-10T00:22:23.498493Z","iopub.status.idle":"2025-03-10T00:22:23.519082Z","shell.execute_reply.started":"2025-03-10T00:22:23.498473Z","shell.execute_reply":"2025-03-10T00:22:23.518525Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def main():\n    required_paths = [\n        TRAIN_CSV,\n        TEST_CSV,\n        TRAIN_DATA_DIR,\n        TEST_DATA_DIR\n    ]\n    \n    for path in required_paths:\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Не найден путь: {path}\")\n            \n    print(\"Все необходимые файлы и директории найдены\")\n    \n    train_df = pd.read_csv(TRAIN_CSV)\n    test_df = pd.read_csv(TEST_CSV)\n    \n    print(f\"Исходный размер тренировочного датасета: {len(train_df)}\")\n    print(f\"Исходный размер тестового датасета: {len(test_df)}\")\n    \n    def check_image_exists(row, data_dir):\n        img_path = os.path.join(data_dir, f\"{row['id']}.jpg\")\n        return os.path.exists(img_path)\n    \n    train_df = train_df[train_df.apply(lambda x: check_image_exists(x, TRAIN_DATA_DIR), axis=1)]\n    test_df = test_df[test_df.apply(lambda x: check_image_exists(x, TEST_DATA_DIR), axis=1)]\n    \n    print(f\"\\nРазмер тренировочного датасета после фильтрации: {len(train_df)}\")\n    print(f\"Размер тестового датасета после фильтрации: {len(test_df)}\")\n    \n    unique_colors = train_df['target'].unique()\n    print(\"\\nУникальные цвета в данных:\")\n    print(unique_colors)\n    \n    unknown_colors = [color for color in unique_colors if color not in TRANSLIT_TO_RU]\n    if unknown_colors:\n        raise ValueError(f\"Найдены неизвестные цвета: {unknown_colors}\")\n    \n    print(\"Все цвета успешно маппятся\")\n    \n    if len(train_df) == 0:\n        raise ValueError(\"После фильтрации не осталось тренировочных данных!\")\n    if len(test_df) == 0:\n        raise ValueError(\"После фильтрации не осталось тестовых данных!\")\n    \n    train_transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.RandomCrop(224),\n        transforms.ToTensor(),\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n    \n    train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['target'])\n    train_dataset = ProductDataset(train_df, TRAIN_DATA_DIR, transform=train_transform)\n    val_dataset = ProductDataset(val_df, TRAIN_DATA_DIR, transform=test_transform)\n    test_dataset = ProductDataset(test_df, TEST_DATA_DIR, transform=test_transform, is_test=True)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Используется устройство: {device}\")\n    \n    model_path = 'best_model.pth'\n    if os.path.exists(model_path):\n        print(\"Загружаем существующую модель...\")\n        model = load_model(model_path)\n        model = model.to(device)\n    else:\n        print(\"Начинаем обучение новой модели...\")\n        model = ColorClassifier(len(COLORS), len(CATEGORIES))\n        model = model.to(device)\n        \n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n        \n        # --- ВАЖНО: теперь train_model возвращает два значения ---\n        best_model_state, history = train_model(\n            model, train_loader, val_loader, criterion, optimizer, scheduler\n        )\n        model.load_state_dict(best_model_state)\n    \n    print(\"Делаем предсказания...\")\n    predictions_df = predict(model, test_loader)\n    predictions_df.to_csv('submission.csv', index=False)\n    print(\"Готово! Результаты сохранены в submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:23.519825Z","iopub.execute_input":"2025-03-10T00:22:23.520039Z","iopub.status.idle":"2025-03-10T00:22:23.537740Z","shell.execute_reply.started":"2025-03-10T00:22:23.520016Z","shell.execute_reply":"2025-03-10T00:22:23.537042Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:22:23.538588Z","iopub.execute_input":"2025-03-10T00:22:23.538825Z","iopub.status.idle":"2025-03-10T00:24:22.597218Z","shell.execute_reply.started":"2025-03-10T00:22:23.538805Z","shell.execute_reply":"2025-03-10T00:24:22.595868Z"}},"outputs":[{"name":"stdout","text":"Все необходимые файлы и директории найдены\nИсходный размер тренировочного датасета: 33303\nИсходный размер тестового датасета: 1434\n\nРазмер тренировочного датасета после фильтрации: 33303\nРазмер тестового датасета после фильтрации: 344\n\nУникальные цвета в данных:\n['zelenyi' 'chernyi' 'belyi' 'bordovyi' 'krasnyi' 'bezhevyi'\n 'raznocvetnyi' 'rozovyi' 'serebristyi' 'korichnevyi' 'fioletovyi' 'seryi'\n 'goluboi' 'oranzhevyi' 'sinii' 'biryuzovyi' 'zolotoi' 'zheltyi']\nВсе цвета успешно маппятся\nНайдено 26642 валидных изображений из 26642\nНайдено 6661 валидных изображений из 6661\nНайдено 344 валидных изображений из 344\nИспользуется устройство: cuda\nНачинаем обучение новой модели...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393dc4d9a5304b40ace16eab9dcece38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d8acb5fc994309848ca7c96ec7e0dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9c44c4388c40da8757ae7e28e2a6ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5043eea1ff4ccea3c6195657ac09c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaea27ae02074065a46743086d82ef4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84006de51ca4e34aac7fb700b70e25b"}},"metadata":{}},{"name":"stdout","text":"Используется устройство: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25:   0%|          | 0/833 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n                                                            \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c7bc734e5e35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-20d3b4ac9bb3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# --- ВАЖНО: теперь train_model возвращает два значения ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         best_model_state, history = train_model(\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         )\n","\u001b[0;32m<ipython-input-7-ffb7ec0d5c7b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-da1ed7c38fe3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, categories)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mimage_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Combine image and text embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-da1ed7c38fe3>\u001b[0m in \u001b[0;36membed_text\u001b[0;34m(self, categories)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Если это тензор, конвертируем в список строк\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Если список, приводим к строковому виду\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}